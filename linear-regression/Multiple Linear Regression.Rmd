---
title: "Multiple Linear Regression"
author: "James "Mac" Stewart"
date: '2023-04-25'
output: html_document
---

# Multiple Linear Regression

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# R

## Necessary Packages

```{R}
library(tidyverse)
library(tidymodels)
library(GGally)
library(reticulate)
library(leaps)
library(forcats)
```

## Importing Data

```{R}
# reading in the data
starbucks <- read.csv("~/R/sta631/language-comparison/linear-regression/Data/starbucks.csv")

# creating factor variables
starbucks$size <- as.factor(starbucks$size)
starbucks$milk <- as.factor(starbucks$milk)
starbucks$whip <- as.factor(starbucks$whip)
```

## Running the Models

The first model that we are going to try is including all of the variables in the data set.

```{R}
# creating the model
lm1 <- lm(calories ~ ., data = starbucks)

# viewing the model
tidy(lm1)
glance(lm1)
```

This model had almost 100% of the variance explained. There were a few variables included that I will want to remove as they seem to not be significant. The first variable that was not significant was caffeine_mg.

```{R}
# creating new dataset
starbucks2 <- starbucks %>% select(-caffeine_mg)

# creating the model
lm2 <- lm(calories ~ ., data = starbucks2)

# viewing the model
tidy(lm2)
glance(lm2)
```

Interestingly enough, the other variable that does not seem to be helpful in predicting the number of calories for the starbucks drink is sugar_g. If we look back at the ggpairs() output in the data cleaning file, you can see that total_carbs_g and sugar_g were extremely correlated with each other. Because they are so correlated, the variance being explained by total_carbs_g might do the majority of the explaining for sugar_g and so it is not helping. Sugar_g will be removed next.

```{R}
# creating the new data set
starbucks2 <- starbucks2 %>% select(-sugar_g)

# creating the model
lm3 <- lm(calories ~ ., data = starbucks2)

# viewing the model
tidy(lm3)
glance(lm3)
```

While it does not seem to be necessary in this model, I think that it is important to look at interaction affects when comparing two different languages. Because of that, I will try to add a few interaction affects to get a better result.

```{R}
lm4 <- lm(calories ~ . + (total_fat_g * cholesterol_mg), data = starbucks2)

# viewing the model
tidy(lm4)
glance(lm4)
```

While the model only seems to be marginally better, the interaction was significant.

## Checking assumptions

Similar to what was done with simple linear regression, we will need to check the assumptiosn for the model.

```{R}
par(mfrow = c(2, 2))
plot(lm4)
```

All of the assumptions seem to be relatively met. The residual versus fitted plot follows the straight horizontal line and seems to be relatively constant in it's variance. Additionally, the QQ plot seems to follow the diagonal line decently well. The biggest issues seem to be the outliers which are having a drastic pull on the model. However, with an r^2 value of ~0.99 I don't think that these are overly important.

# python

## Necessary Packages

```{python}
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
```

## Importing Data

```{python}
# reading in the data
starbucks = pd.DataFrame(r.starbucks)

# dropping the X variable
starbucks = starbucks.drop("X", axis = 1)

# creating categorical variables
starbucks[['size', 'milk']] = starbucks[['size', 'milk']].astype('category')
```

## Running the Models

In this section, I want to take another approach at creating the model in python. Instead of using Scikit Learn's packages, i will attempt to use the statsmodels package. The first thing we will need to do is utilize pandas get_dummies() to create dummy variables for the ones that are categorical.

```{python}
starbucks = pd.get_dummies(starbucks)
```

Next, we will try to run the full model.

```{python}
# creating the variables
X = starbucks.drop('calories', axis = 1)
y = starbucks[['calories']]

# creating the model
lm1 = sm.OLS(y, X).fit()
lm1.summary()
```

As you can see, the output from this model is significantly more in-depth and closer to the output produced by the r code. It is also simpler to use than the scikit learn packages. There do appear to be some differences in the results however. This is most likely due to how we handled the categorical variables in this dataset. For instance, caffeine appears to be a significant predictor in this model where it was not in the R output.

There are some variables that don't seem to be significant (sugar_g and sodium_mg) which should be addressed. We will first remove sugar_g.

```{python}
# creating the variables
X = starbucks.drop(['calories', 'sugar_g'], axis = 1)
y = starbucks[['calories']]

# creating the model
lm2 = sm.OLS(y, X).fit()
lm2.summary()
```

This didn't make much of a difference between the models so we will continue and drop sodium_mg.

```{python}
# creating the variables
X = starbucks.drop(['calories', 'sugar_g', 'sodium_mg'], axis = 1)
y = starbucks[['calories']]

# creating the model
lm3 = sm.OLS(y, X).fit()
lm3.summary()
```

Once again, this didn't seem to make much of a difference between the models but there are no longer any single variables that are not significant. Because of this, we will continue on using this model.

For testing interaction effects, in python you will have to create the column with the interaction. An example with the same relationship shown in the r code can be seen below.

```{python}
# creating the interaction affect
starbucks['fat_x_chol'] = starbucks['total_fat_g'].multiply(starbucks['cholesterol_mg'], axis = 'index')

# creating the variables
X = starbucks.drop(['calories', 'sugar_g', 'sodium_mg'], axis = 1)
y = starbucks[['calories']]

# creating the model
lm4 = sm.OLS(y, X).fit()
lm4.summary()
```

While the interaction does seem to be a significant predictor, it didn't seem to affect the model's r^2 value like it did in the r version. Because of that, It will be better to continue with the simpler model.

Creating an interaction affect did seem to be a bit more tedious in the python version. It required you to create an additional variable inside the dataset instead of just including it in the model and having the function make the calculations for you. This might speed up the process in the long term however.

## Checking Assumptions

This process will be quite similar to what was done in simple linear regression. As such, I will do a little less explaining here.

```{python}
# creating predictions
predictions = lm3.predict()

## Residual vs Fitted plot
# clearing figure layouts
plt.clf()
plt.cla()

# creating plots
sns.residplot(x=predictions.reshape(-1), y='calories', data = starbucks, lowess = True, line_kws={'color': 'red', 'lw': 1, 'alpha':1})

# setting axis labels, title, and layout
plt.xlabel("Fitted values")
plt.title("Residual")
plt.tight_layout()

# printing visual.
plt.show()

## Q-Q Plot
# clearing figure layouts
plt.clf()
plt.cla()

# getting residuals
residuals = starbucks['calories'] - predictions.reshape(-1)

# creating figure
scipy.stats.probplot(residuals, dist = 'norm', plot = plt)

# setting title and layout
plt.tight_layout()
plt.title("Normal Q-Q Plot")

# printing visual
plt.show()
```

The output from these graphs are very similar to what we had seen in the r output with not much more to say.

# Overall Assessment

Based on what we did in this program, I think that there is not much of a difference between using r or python. The statsmodels library created much more detailed output that what we had seen with scikit learn and is even in a more aesthetically pleasing format than that of r. However, created interaction affects is cumbersome and requires storing more information in memory which is a bigger drawback. Finally, the visual representation is not improved with the statsmodels library so the simplicity of R to visualize their plots is a big bonus.

