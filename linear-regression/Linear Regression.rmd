---
title: "Linear Regression - R"
author: "James 'Mac' Stewart"
date: '2023-03-02'
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


## Necessary Packages
```{R}
library(tidyverse)
library(tidymodels)
library(GGally)
library(reticulate)
library(leaps)
library(forcats)
```

## Importing Data

The following code chunk was taken from [tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-25/readme.md) where we pull in data. I chose to use their data cleaning code as this is not the main goal of the project.

```{R}
ratings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv')
details <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv')
```

# Begin Language Comparison

This section will go through the process of creating linear regression models. Because this is the first model that we are going through in the project, I will go through the data cleaning in both R and Python to show how easy it is for both. Each subsection will be put into two parts. First, you will see it done in R and then you will see it done in python.

As noted, the python parts will utilize the reticulate package.

## Cleaning Data - R

After pulling in the two included data sets, I then want to combine them into a single flat file. I am pulling in only the average variable from the ratings data set. I will also be dropping a few of the variables included in the details tab as they will not be used for the scope of this project.

```{R}
ratings_small <- ratings %>% select(id, name, average)
details_small <- details %>% select(-c(num, description, wishing, wanting, trading, owned, boardgameexpansion, boardgameimplementation))
```

This function allows us to pull the first part of each of the listed variables like boardgamecategory. While the full list would be more beneficial, the first listed item in these categories are enough for the scope of this project.

```{R}
get_first_part <- function(word){
  list = str_split(word, "'")
  first_word = list[[1]][2]
  return(first_word)
}
```

We will then apply this function to the variables boardgamecategory, boardgamemechanic, boardgamefamily, boardgamedesigner, boardgameartist, and boardgamepublisher. It is important that, if run multiple times, that you apply the function on the original data set columns. Otherwise, it will not find error out on the next run.

```{R}
details_small$boardgamecategory <- details %>%
  select(boardgamecategory) %>%
  apply(1, get_first_part)

details_small$boardgamemechanic <- details %>%
  select(boardgamemechanic) %>%
  apply(1, get_first_part)

details_small$boardgamefamily <- details %>%
  select(boardgamefamily) %>%
  apply(1, get_first_part)

details_small$boardgamedesigner <- details %>%
  select(boardgamedesigner) %>%
  apply(1, get_first_part)

details_small$boardgameartist <- details %>%
  select(boardgameartist) %>%
  apply(1, get_first_part)

details_small$boardgamepublisher <- details %>%
  select(boardgamepublisher) %>%
  apply(1, get_first_part)
```

We need to join the two tables together to create the final data set and make the character variables factors.

```{R}
final_data <- merge(details_small, ratings_small, "id") %>%
  select(-primary, -id)

final_data$boardgameartist <- as.factor(final_data$boardgameartist)
final_data$boardgamecategory <- as.factor(final_data$boardgamecategory)
final_data$boardgamedesigner <- as.factor(final_data$boardgamedesigner)
final_data$boardgamefamily <- as.factor(final_data$boardgamefamily)
final_data$boardgamemechanic <- as.factor(final_data$boardgamemechanic)
final_data$boardgamepublisher <- as.factor(final_data$boardgamepublisher)
```

For the factor variables, we will want to check how many levels there are and simplify them down so that dummy variables created later will be easier to make. We utilize the fct_other() function in the forcats package.

```{R}
### boardgamecategory
#getting frequency tables for each variable
table = count(final_data, boardgamecategory)
table %>%
  arrange(desc(n))

# there are 83 levels of this factor. We are going to keep the top 5 and turn everything else into other
# top 5 num > 1000
final_data <- final_data %>%
  mutate(
    category = fct_other(boardgamecategory, keep = c("Card Game", "Abstract Strategy", "Adventure", "Action / Dexterity", "Animals"), other_level = "Other"))

#verifying this is working
table = count(final_data, category)
table %>%
  arrange(desc(n))

### boardgamepublisher
table = count(final_data, boardgamepublisher)
table %>%
  arrange(desc(n))

#keeping levels over 200
over200 <- c("(Self-Published)", "(Web published)", "Hasbro", "Decision Games (I)", "GMT Games", "Ravensburger", "AMIGO", "KOSMOS", "(Public Domain)", "999 Games", "Asmodee")
final_data <- final_data %>%
  mutate(publisher = fct_other(boardgamepublisher, keep = over200, other_level = "Other"))

### boardgameartist
table = count(final_data, boardgameartist)
table %>%
  arrange(desc(n))

#keeping anything over 100
over100 <- c("(Uncredited)", "Franz Vohwinkel", "Redmond A. Simonsen", "Michael Menzel", "Klemens Franz", "Oliver Freudenreich", "NicolÃ¡s Eskubi")
final_data <- final_data %>%
  mutate(artist = fct_other(boardgameartist, keep = over100, other_level = "Other"))

### boardgamefamily
table = count(final_data, boardgamefamily)
table %>%
  arrange(desc(n))

# keeping everything over 200
over200 <- c("Crowdfunding: Kickstarter", "Admin: Better Description Needed!", "Players: Two Player Only Games", "Category: Combinatorial", "Components: Miniatures")
final_data <- final_data %>%
  mutate(family = fct_other(boardgamefamily, keep = over200, other_level = "Other"))

### boardgamemechanic
table = count(final_data, boardgamemechanic)
table %>%
  arrange(desc(n))

# keeping everythin over 1000
over1000 = c("Dice Rolling", "Hand Management", "Area Majority / Influence", "Card Drafting", "Action Points", "Cooperative Game")
final_data <- final_data %>%
  mutate(mechanic = fct_other(boardgamemechanic, keep = over1000, other_level = "Other"))

### boardgamedesigner
table = count(final_data, boardgamedesigner)
table %>%
  arrange(desc(n))

# keeping everything over 100
over100 <- c("(Uncredited)", "Reiner Knizia", "Richard H. Berg", "Joseph Miranda")
final_data <- final_data %>%
  mutate(designer = fct_other(boardgamedesigner, keep = over100, other_level = "Other"))
```

Now that I have narrowed down the factor variables, I am going to drop the unedited factor vars from the dataset

```{R}
final_data <- final_data %>%
  select(-boardgamecategory, -boardgamemechanic, -boardgamefamily, -boardgameartist, -boardgamepublisher, -boardgamedesigner, -name)
```

## Cleaning Data - Python

```{python}
# importing padas package
import pandas  as pd

# getting the datasets into pandas dataframes and selecting the specific variables
ratings = pd.DataFrame(r.ratings) # you use 'r.' to access objects held in the R working library
ratings_small = ratings[['id', 'name', 'average']]

details = pd.DataFrame(r.details)
details_small = details.drop(['num', 'description', 'wishing', 'wanting', 'trading', 'owned', 'boardgameexpansion', 'boardgameimplementation'], axis = 1)
```

For my first time using reticulate and access different objects, it seems to be pretty intuitive. I used an [article](https://rstudio.github.io/reticulate/) on the rstudio github page to start with the basics.

In terms of comparing, I personally like working in pandas and the way they access/modify data frames in terms like this. However, I think the tidy data frame pipelines are going to be more efficient and intuitive which we will see later.

```{python}
def get_first_part(word):
  words = word.replace("[","").split(",")
  return words[0]
```

In terms of making basic functions, python seems to be more intuitive. The actual text altering seemed relatively the same, although python views strings as a list which could make it slightly easier for some more complex modifications.

```{python}
details_small['boardgamecategory'] = [get_first_part(x) for x in details_small['boardgamecategory']]
details_small['boardgamemechanic'] = [get_first_part(x) for x in details_small['boardgamemechanic']]
details_small['boardgamefamily'] = [get_first_part(x) for x in details_small['boardgamefamily']]
details_small['boardgamedesigner'] = [get_first_part(x) for x in details_small['boardgamedesigner']]
details_small['boardgameartist'] = [get_first_part(x) for x in details_small['boardgameartist']]
details_small['boardgamepublisher'] = [get_first_part(x) for x in details_small['boardgamepublisher']]
```

I think that python's list comprehension is really neat and has a lot more use than the functions we have here. However, with how simplistic this type of function is, I think R dplyr's apply() function is probably better.

```{python}
final_data = details_small.merge(ratings_small, how = 'left', left_on = 'id', right_on = 'id').drop(['id', 'primary'], axis = 1)
```

In this simplistic example, I don't think there is much of a difference between python and r when it comes to merging dataframes. You have to supply more information with pandas merge, but not enough to where it makes a difference.

## Graphing the Variables - R

We will then graph each of the independent variables against the dependent variable. This will allow us to see which ones might have a correlation which will aide in choosing which factors to include in our analysis. Due to the large number of possibilities for the character variables, I will only be graphing the relationships with the numeric variables.

```{R}
final_data %>%
  select(average, yearpublished, minplayers, maxplayers, playingtime, minplaytime, maxplaytime, minage) %>%
  ggpairs()
```

It looks like maxplaytime and playingtime are perfectly correlated so we will drop one of these variables to keep all the variables reasonably indepednet.

```{R}
final_data <- final_data %>%
  select(-maxplaytime)
```

## Graphing the Variables - Python

I was a little worried about this step. However a stack overflow [post](https://stackoverflow.com/questions/68967458/is-there-is-an-equivalent-of-rs-ggallyggpairs-function-in-python) recommended using Seaborn's pairgrid package.

```{python}
# reaidng packages
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# creating a function to get correlation
def correlation(x,y, **kwargs):
    ax = plt.gca()
    r, p = pearsonr(x, y)
    ax.annotate('r = {}'.format(r), 
                xy=(0.5, 0.5), #setting the dimension of the output
                xycoords='axes fraction', #tells the value to print based on the coordinates of the box
                ha='center') #specifying horizontal alignment
    ax.set_axis_off()

# creating the pairgrid for all of the following variables
g = sns.PairGrid(final_data[['average', 'yearpublished', 'minplayers', 'maxplayers', 'playingtime', 'minplaytime', 'maxplaytime', 'minage']])
g.map_upper(correlation) #setting the top boxes to print out the correlation created from our function
g.map_diag(sns.histplot) # setting the boxes on the diagonal to be histograms
g.map_lower(sns.scatterplot) #setting the bottom boxes to be scatterplots

fig = g.fig #saving the output as a figure so it can be saved

fig.savefig("./pair_grid.png") #saving the figure. This is assuming the current working directory is the linear-regression folder
```

There are a couple of notes. First off, python's version of the ggally:ggpair() with seaborn's parigrid is a lot more complicated. However, I can see that there are a lot of options to modify different aspects of the results. It required consulting different posts [here](https://stackoverflow.com/questions/66893103/pairgrid-use-upper-triangle-for-correlations-access-full-data) and [here](https://stackoverflow.com/questions/32244753/how-to-save-a-seaborn-plot-into-a-file) to reproduce what is almost immediate for the r package. However, the output in python is drastically better in quality. So if a more in-depth look at the visuals is required it might be worth using python's version. But, for any quick look, r's version will definitely be preferred.

## Simple Linear Regression - R

It appears that minage has the greatest (negative) correlation with the average rating score. Because of this, we will use this variable for our simple linear regression.

```{R}
slrm1 <- lm(average ~ minage, data = final_data)

tidy(slrm1)
glance(slrm1)
```

With an r^2 value of ~0.037, a simple linear regression does not seem to be appropriate to model the relationship between the independent and dependent variables. Because of this, a multiple linear regression will be created.

## Simple Linear Regression - Python

Most or all of the python model creation will be done using scikit learn's modeling packages. Specifically, they have an overarching package for regression which will be utilized heavily.

```{python}
from scikitlearn.linear_model import LinearRegression as LR
from scikitlern.metrics import r2_score

# creating and fitting the model
slr1 = LR().fit(x = final_data['minage'], y = final_data['average'])

# getting the r^2 score of the model
predictions = slr1.predict(final_data['minage']) # getting predictions
score = r2_score(final_data['average'], predictions) # getting r2_score

print(score) # printing out the score
```

***I am having trouble getting reticulate to print out the stuff from the scikit learn programs. Because of this, I am going to continue on with the R portion and come back to the python work***

## Multiple Linear Regression - R

When doing modeling with multiple independent variables, you will often need to find a subset of these variables that will be useful. To do this we are going to first run the model with all variables to see what happens.

As a reminder the dependent variables available to us are:
  - yearpublished
  - minplayers
  - maxplayers
  - playingtime
  - minplaytime
  - minage
  - category (factor)
  - mechanic (factor)
  - family (factor)
  - designer (factor)
  - artist (factor)
  - publisher (factor)


```{R}
# running the full model with all variable
mlr1 <- lm(average ~ ., data = final_data)
tidy(mlr1)
glance(mlr1)
```

This gave us an r^2 value considerably better than the simple linear regression model that we created previously. Additionally, all of the dependent variable seem to be significant predictors of the average score. We will need to check the assumptions of this model using QQ plots.

## Checking Assumptions - R

```{R}
plots = plot(mlr1)
```

The assumptions of linear regression are as follows.

  1. Linear Relationship
    - We can judge this using the fitted vs residuals plot. As you can see, there does seem to be some sort of relationship following the line in the center. There are some deviations however in the outliers.
  2. Independence
    - We judge this based on the way we collect the variables. While we don't know for sure if the average score people give is affected at all by other board games, we can assume that this noise is canceled out by law of large numbers.
  3. Constant Variance
    - Here is where the biggest issue is with the model. The variance kind of spreads out towards the middle of the pack and then shrinks as you go towards the end. Because of this we can attempt to transform the average score which will be done below.
  4. Normality
    - Here we can asses this using the QQ plot provided. There is a slight curve from the line towards the outliers, but the center really follows a straight line. This will be reassessed after the transformation from constant variance to see if that makes any difference.
    
    
```{R}
### taking the log of the dependent variable
mlr2 <- lm(log(average) ~ ., data = final_data)
glance(mlr2)

plot(mlr2)

# this caused a worse model and the QQ plot to be less normal

### taking the square root of the dependent variable
mlr3 <- lm(sqrt(average) ~ ., data = final_data)
glance(mlr3)

plot(mlr3)

# this did not much better

### taking the square of the dependent variable
mlr4 <- lm(average**2 ~ ., data = final_data)
glance(mlr4)

plot(mlr4)

# this did slightly better and would be the model we use


```
  
Finally, we will need to test for multi-colinearity because it is a multiple linear regression



