---
title: "Linear Regression - R"
author: "James 'Mac' Stewart"
date: '2023-03-02'
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


## Necessary Packages
```{R}
library(tidyverse)
library(tidymodels)
library(GGally)
library(reticulate)
library(leaps)
library(forcats)
```

## Importing Data

The following code chunk was taken from [tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-25/readme.md) where we pull in data. I chose to use their data cleaning code as this is not the main goal of the project.

```{R}
ratings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv')
details <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv')
```

# Begin Language Comparison

This section will go through the process of creating linear regression models. Because this is the first model that we are going through in the project, I will go through the data cleaning in both R and Python to show how easy it is for both. Each subsection will be put into two parts. First, you will see it done in R and then you will see it done in python.

As noted, the python parts will utilize the reticulate package.

## Cleaning Data - R

After pulling in the two included data sets, I then want to combine them into a single flat file. I am pulling in only the average variable from the ratings data set. I will also be dropping a few of the variables included in the details tab as they will not be used for the scope of this project.

```{R}
ratings_small <- ratings %>% select(id, name, average)
details_small <- details %>% select(-c(num, description, wishing, wanting, trading, owned, boardgameexpansion, boardgameimplementation))
```

This function allows us to get a frequency table of all the words that are in the category so that we can pick out the top ones to include as variables.

```{R}
# function for getting the top words
get_top_words <- function(data, var){
  # create dictionary
  words <- c()
  
  # cycle through data variable
  for(i in 1:nrow(data)){
    
    # get list of words for the categorical variable
    string = str_replace_all(data[[var]][i], c("', ","\""), '')
    list <- str_split(string, "'")
    
    list = list[!is.na(list)]
    if(length(list) >= 1){
      # cycle through list of words
      for(i in 1:(length(list[[1]]))){
        if(list[[1]][i] %in% names(words)){
          words[list[[1]][i]] = words[list[[1]][i]] + 1
        } else {
          words[list[[1]][i]] = 1
        }
      }
    }
    
  }
  
  words = words[order(unlist(words), decreasing = TRUE)]
  return(words[3:20])
  
}
```

Below, I run the above function on each of the categorical variables and then select the specific variables in a list that I want to check.

```{R}
### boardgamecategory
print(get_top_words(details, "boardgamecategory"))
# I'm going to use the top 8 categories along with adventure and puzzle
categories <- c("Card Game", "Wargame", "Fantasy", "Party Game", "Dice", "Science Fiction", "Fighting", "Abstract Strategy", "Adventure", "Puzzle")

### boardgamemechanic
print(get_top_words(details, "boardgamemechanic"))
# i kept the top 10 and then cooperative as this seemed like a bad one to leave out
mechanics <- c("Dice Rolling", "Hand Management", "Set Collection", "Variable Player Powers", "Hexagon Grid", "Simulation", "Card Drafting", "Tile Placement", "Modular Board", "Grid Movement", "Cooperative Game")

### boardgamefamily
print(get_top_words(details, "boardgamefamily"))
# there was a significant drop off after the third family so I only kept those
families <- c("Players: Two Player Only Games", "Crowdfunding: Kickstarter", "Players: Games with Solitaire Rules")

### boardgamedesinger
print(get_top_words(details, "boardgamedesigner"))
# there was no real cutoff after the first one so I'm only going to keep everything over 100
# for this one I kept (Uncredited) as I feel like this might be a genre of indie designers on its own
designers <- c("(Uncredited)", "Reiner Knizia", "Joseph Miranda", "Wolfgang Kramer", "Richard H. Berg")

### boardgameartist
print(get_top_words(details, "boardgameartist"))
# similar there didn't seem to be any huge cut off after the first one so I'm keeping everything over 200
artists <- c("Rodger B. MacGowan", "(Uncredited)", "Franz Vohwinkel", "Redmond A. Simonsen", "Mark Simonitch")

### boardgamepublisher
print(get_top_words(details, "boardgamepublisher"))
# Once again, there isn't a stark cut off
publishers <- c("Hasbro", "(Self-Published)", "Asmodee", "Ravensburger", "Parker Brothers", "(Web published)", "Pegasus Spiele", "999 Games", "Korea Boardgames Co., Ltd.", "KOSMOS", "Hobby World", "Edge Entertainment")


```

I am now going to create a function that will create a variable i a specific dataset using a string and then cycle through each of the rows to see if they contain that specific string.

```{R}
# creating the function
create_variable_count <- function(var1, var2){
  # creates the new variable
  details_small[[var1]] <<- 0
  
  # iterate through each row of the second variable
  for(i in 1:nrow(details_small)){
    # if string 1 is contained in string 2 then 
    if(grepl(var1, details_small[[var2]][i])){
      # set the value of the new variable on that row to equal 1
      details_small[[var1]][i] <<- 1
    }
  }
}
```


I am now going to apply this function to each of the lists created in the prior section for each of the categorical variables.

```{R}
# categories
for(i in categories){
  create_variable_count(i, "boardgamecategory")
}

# artists
for(i in artists){
  create_variable_count(i, "boardgameartist")
}

# designers
for(i in designers){
  create_variable_count(i, "boardgamedesigner")
}

# families
for(i in families){
  create_variable_count(i, "boardgamefamily")
}

# mechanics
for(i in mechanics){
  create_variable_count(i, "boardgamemechanic")
}

# publishers
for(i in publishers){
  create_variable_count(i, "boardgamepublisher")
}

```

We need to join the two tables together to create the final data set and make the. We also need to make sure that the variables created are now factor variables

```{R}
# merging the dataset
final_data <- merge(details_small, ratings_small, "id") %>%
  select(-primary, -id, -name, -boardgamecategory, -boardgamemechanic, -boardgamefamily, -boardgamedesigner, -boardgameartist, -boardgamepublisher)

# creating the factor variables
vars <- c(8:52)
final_data[,vars] <- lapply(final_data[,vars], factor)
```


## Cleaning Data - Python

```{python}
# importing padas package
import pandas  as pd

# getting the datasets into pandas dataframes and selecting the specific variables
ratings = pd.DataFrame(r.ratings) # you use 'r.' to access objects held in the R working library
ratings_small = ratings[['id', 'name', 'average']]

details = pd.DataFrame(r.details)
details_small = details.drop(['num', 'description', 'wishing', 'wanting', 'trading', 'owned', 'boardgameexpansion', 'boardgameimplementation'], axis = 1)
```

For my first time using reticulate and access different objects, it seems to be pretty intuitive. I used an [article](https://rstudio.github.io/reticulate/) on the rstudio github page to start with the basics.

In terms of comparing, I personally like working in pandas and the way they access/modify data frames in terms like this. However, I think the tidy data frame pipelines are going to be more efficient and intuitive which we will see later.

```{python}
def get_first_part(word):
  words = word.replace("[","").split(",")
  return words[0]
```

In terms of making basic functions, python seems to be more intuitive. The actual text altering seemed relatively the same, although python views strings as a list which could make it slightly easier for some more complex modifications.

```{python}
details_small['boardgamecategory'] = [get_first_part(x) for x in details_small['boardgamecategory']]
details_small['boardgamemechanic'] = [get_first_part(x) for x in details_small['boardgamemechanic']]
details_small['boardgamefamily'] = [get_first_part(x) for x in details_small['boardgamefamily']]
details_small['boardgamedesigner'] = [get_first_part(x) for x in details_small['boardgamedesigner']]
details_small['boardgameartist'] = [get_first_part(x) for x in details_small['boardgameartist']]
details_small['boardgamepublisher'] = [get_first_part(x) for x in details_small['boardgamepublisher']]
```

I think that python's list comprehension is really neat and has a lot more use than the functions we have here. However, with how simplistic this type of function is, I think R dplyr's apply() function is probably better.

```{python}
final_data = details_small.merge(ratings_small, how = 'left', left_on = 'id', right_on = 'id').drop(['id', 'primary'], axis = 1)
```

In this simplistic example, I don't think there is much of a difference between python and r when it comes to merging dataframes. You have to supply more information with pandas merge, but not enough to where it makes a difference.

## Graphing the Variables - R

We will then graph each of the independent variables against the dependent variable. This will allow us to see which ones might have a correlation which will aide in choosing which factors to include in our analysis. Due to the large number of possibilities for the character variables, I will only be graphing the relationships with the numeric variables.

```{R}
final_data %>%
  select(average, yearpublished, minplayers, maxplayers, playingtime, minplaytime, maxplaytime, minage) %>%
  ggpairs()
```

It looks like maxplaytime and playingtime are perfectly correlated so we will drop one of these variables to keep all the variables reasonably indepednet.

```{R}
final_data <- final_data %>%
  select(-maxplaytime)
```

There is an extreme outlier regarding playingtime that I believe should be removed as it looks like an error. Additionally, there is an average score above 10 which is false data.There are also games with playingtime of 0. However, there are so many of them that it does not seem correct to remove them. This error will most likely cause issues, especially as it looks like there seems to be an exponential relationship between playingtime and average score.

```{R}
final_data <- final_data %>%
  filter(average <= 10) %>%
  filter(playingtime <= 30000)
```

It also looks like there are some exponential relationships between some of the independent and dependent variables. I will do a log function to try to linearize this. It is important to note, that there are 0 values in here. Because of this, all of the datapoints will be increased by 1 so that they are able to be used.

```{R}
final_data$log_playingtime <- log(final_data$playingtime + 1)
final_data$log_minplaytime <- log(final_data$minplaytime + 1)

## checking to see if the relationship improved at all
final_data %>%
  select(average, playingtime, log_playingtime, minplaytime, log_minplaytime) %>%
  ggpairs()

```

While this doenst look the best, there does seem to be some improvement, so we will continue with these variables.

## Graphing the Variables - Python

I was a little worried about this step. However a stack overflow [post](https://stackoverflow.com/questions/68967458/is-there-is-an-equivalent-of-rs-ggallyggpairs-function-in-python) recommended using Seaborn's pairgrid package.

```{python}
# reaidng packages
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# creating a function to get correlation
def correlation(x,y, **kwargs):
    ax = plt.gca()
    r, p = pearsonr(x, y)
    ax.annotate('r = {}'.format(r), 
                xy=(0.5, 0.5), #setting the dimension of the output
                xycoords='axes fraction', #tells the value to print based on the coordinates of the box
                ha='center') #specifying horizontal alignment
    ax.set_axis_off()

# creating the pairgrid for all of the following variables
g = sns.PairGrid(final_data[['average', 'yearpublished', 'minplayers', 'maxplayers', 'playingtime', 'minplaytime', 'maxplaytime', 'minage']])
g.map_upper(correlation) #setting the top boxes to print out the correlation created from our function
g.map_diag(sns.histplot) # setting the boxes on the diagonal to be histograms
g.map_lower(sns.scatterplot) #setting the bottom boxes to be scatterplots

fig = g.fig #saving the output as a figure so it can be saved

fig.savefig("./pair_grid.png") #saving the figure. This is assuming the current working directory is the linear-regression folder
```

There are a couple of notes. First off, python's version of the ggally:ggpair() with seaborn's parigrid is a lot more complicated. However, I can see that there are a lot of options to modify different aspects of the results. It required consulting different posts [here](https://stackoverflow.com/questions/66893103/pairgrid-use-upper-triangle-for-correlations-access-full-data) and [here](https://stackoverflow.com/questions/32244753/how-to-save-a-seaborn-plot-into-a-file) to reproduce what is almost immediate for the r package. However, the output in python is drastically better in quality. So if a more in-depth look at the visuals is required it might be worth using python's version. But, for any quick look, r's version will definitely be preferred.

## Simple Linear Regression - R

It appears that minage has the greatest (negative) correlation with the average rating score. Because of this, we will use this variable for our simple linear regression.

```{R}
slrm1 <- lm(average ~ minage, data = final_data)

tidy(slrm1)
glance(slrm1)
```

With an r^2 value of ~0.037, a simple linear regression does not seem to be appropriate to model the relationship between the independent and dependent variables. Because of this, a multiple linear regression will be created.

## Simple Linear Regression - Python

Most or all of the python model creation will be done using scikit learn's modeling packages. Specifically, they have an overarching package for regression which will be utilized heavily.

```{python}
from scikitlearn.linear_model import LinearRegression as LR
from scikitlern.metrics import r2_score

# creating and fitting the model
slr1 = LR().fit(x = final_data['minage'], y = final_data['average'])

# getting the r^2 score of the model
predictions = slr1.predict(final_data['minage']) # getting predictions
score = r2_score(final_data['average'], predictions) # getting r2_score

print(score) # printing out the score
```

***I am having trouble getting reticulate to print out the stuff from the scikit learn programs. Because of this, I am going to continue on with the R portion and come back to the python work***

## Multiple Linear Regression - R

When doing modeling with multiple independent variables, you will often need to find a subset of these variables that will be useful. To do this we are going to first run the model with all variables to see what happens.

As a reminder the dependent variables available to us are:
  - yearpublished
  - minplayers
  - maxplayers
  - playingtime
  - minplaytime
  - minage
  - categories[1:10] (factor)
  - mechanics[1:11] (factor)
  - families[1:3] (factor)
  - designers[1:5] (factor)
  - artists[1:5] (factor)
  - publishers[1:12] (factor)


```{R}
# running the full model with all variable
mlr1 <- lm(average ~ ., data = final_data)
tidy(mlr1)
glance(mlr1)
```

This gave us an r^2 value considerably better than the simple linear regression model that we created previously. However, there are many variables in here that don't seem to be statistically significant. Because of this, we might want to try some feature selection methods on our model.

### backward selection

Backward selection removes variables until the best model is created.

```{R}
models_back <- stats::step(mlr1, direction = "backward")
```

Now we will look at the model.

```{R}
glance(models_back)
tidy(models_back)
```


### forward selection

Forward selection adds variables to the model until the best model is created.

```{R}
models_for <- stats::step(mlr1, direction = "forward")
```

Now we will look at the model

```{R}
glance(models_for)
tidy(models_for)
```

While both models seem to be about the same in terms of r^2 and adjusted r^2, the backwards selection seems to be simpler so that is the base model that we will use.

### Interaction Effects

Now that we have a base selection of variables, I want to see if any interaction affects can be included to make the model better.




